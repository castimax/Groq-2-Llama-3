import os
from fastapi import FastAPI, Request
from langchain_groq import GroqModel
from langchain_community.embeddings import OllamaEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.document_loaders import CSVLoader, JSONLoader
from dotenv import load_dotenv
from googletrans import Translator
from langchain_community.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper
from langchain_community.tools import ArxivQueryRun
from langchain_community.utilities import ArxivAPIWrapper

load_dotenv()

app = FastAPI()

os.environ['GROQ_API_KEY'] = os.getenv("GROQ_API_KEY")
groq_api_key = os.getenv('GROQ_API_KEY')

model_name = "EleutherAI/gpt-j-6B"
model = GroqModel(model_name=model_name, groq_api_key=groq_api_key)

prompt = ChatPromptTemplate.from_template(
    """
    Answer the questions based on the context provided.
    Provide the most accurate answer based on the question.
    <context>
    {context}
    </context>
    Questions: {input}
    """
)

@app.on_event("startup")
async def startup_event():
    embeddings = OllamaEmbeddings()

    pdf_loader = PyPDFDirectoryLoader("./ruta/al/directorio/pdf")
    pdf_docs = pdf_loader.load()

    csv_loader = CSVLoader("./ruta/al/archivo.csv")
    csv_docs = csv_loader.load()

    json_loader = JSONLoader("./ruta/al/archivo.json")
    json_docs = json_loader.load()

    docs = pdf_docs + csv_docs + json_docs

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    final_documents = text_splitter.split_documents(docs)
    vectors = FAISS.from_documents(final_documents, embeddings)

    app.state.vectors = vectors

    wikipedia_api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=200)
    app.state.wikipedia_tool = WikipediaQueryRun(api_wrapper=wikipedia_api_wrapper)

    arxiv_api_wrapper = ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)
    app.state.arxiv_tool = ArxivQueryRun(api_wrapper=arxiv_api_wrapper)

@app.post("/ask")
async def ask_question(request: Request):
    prompt1 = await request.json()
    prompt1 = prompt1["question"]

    translator = Translator()
    prompt1_en = translator.translate(prompt1, dest='en').text

    tools = [app.state.wikipedia_tool, app.state.arxiv_tool]
    
    # Crear el agente utilizando el modelo GPT-J con Groq
    agent = create_custom_tools_agent(model, tools, prompt)
    
    response = agent.invoke({'input': prompt1_en})

    response_es = translator.translate(response['output'], dest='es').text

    return {"respuesta": response_es, "contexto": response["intermediate_steps"]}
